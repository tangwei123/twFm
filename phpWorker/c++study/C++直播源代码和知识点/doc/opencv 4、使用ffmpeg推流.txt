在将RGB转化成YUV，并且经过h264编码之后，就需要推流。

	一、av_register_all();//初始化所有的封装器，只需要一次

	二、avformat_network_init();//注册所有的网络协议，只需要一次
	
	三、创建封装器上下文 //只要一次
		
		AVFormatContext *octx = NULL;		

		int formatCon = avformat_alloc_output_context2(&octx, NULL, “flv”, “rtmp://120.26.204.73/live”);//需要判断是否创建出错

	四、添加视频流：//只要一次.可以通过 codecCon->codec_type == AVMEDIA_TYPE_AUDIO或者AVMEDIA_TYPE_VIDEO判断是音频还是视频。codecCon是之前的编码器上下文。

		AVStream *outstream = avformat_new_stream(octx, codecCon->codec);//这儿需要判断是否出错。codecCon是之前的编码器上下文。

		outstream->codecpar->codec_tag = 0;

		avcodec_parameters_from_context(outstream->codecpar, codecCon);//从编码器复制参数到视频流信息 codecCon为之前格式转换的编码器上下文

		av_dump_format(octx, 0, “rtmp://120.26.204.73/live”, 1);

		可以通过 outstream->index 拿到新增流信息的索引

	五、打开网络IO：//只要一次
		
		avio_open(&octx->pb, “rtmp://120.26.204.73/live”, AVIO_FLAG_WRITE);//需要判断有没有成功

	六、写入封装头：//只要一次
		
		avformat_write_header(octx, 0);//写入封装头，可以提前告诉服务端这个视频流的帧率、分辨率等  需要判断是否成功	需要注意的是，这一步，他会把我们原视频的codec中的time_base改掉，所以我们在推流过程中，需要重新计算每一帧的pts、dts、duration等信息 或者是 直接拿你编码器中的time_base

	七、推流：//循环推流
y
		首先要对packet的stream_index进行赋值，具体怎么赋值看情况，因为默认推流packet的stream_index是默认为0的。然后使用 packet.stream_index 这个参数与 上面的outstream->index做比较，来判断是音频流还是视频流。

		packet.pts = av_rescale_q(packet.pts, cv->time_base, out->time_base);//重新计算pts，这一行的意思是，原本的packet.pts是基于cv（编码器上下文）的time_base，要改成基于输出流的time_base
                
        packet.dts = av_rescale_q(packet.dts, cv->time_base, out->time_base);//重新计算dts，这一行的意思是，原本的packet.dts是基于cv（编码器上下文）的time_base，要改成基于输出流的time_base

		av_interleaved_write_frame(octx, &packet);//这儿会将上面的packet自动释放掉

